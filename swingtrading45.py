import streamlit as st
import yfinance as yf
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import pytz
from datetime import datetime, timedelta
from scipy.signal import argrelextrema

# -----------------------------------------------------------------------------
# 1. SYSTEM CONFIGURATION & STYLING
# -----------------------------------------------------------------------------
st.set_page_config(
    page_title="AI Quant Assistant",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# Professional Dark Theme CSS
st.markdown("""
<style>
    .stApp { background-color: #0b0e11; color: #e0e0e0; font-family: 'Roboto', sans-serif; }
    .stButton>button { width: 100%; background-color: #2962ff; color: white; border-radius: 4px; border: none; font-weight: 600; padding: 0.75rem; transition: all 0.3s; }
    .stButton>button:hover { background-color: #0039cb; box-shadow: 0 4px 12px rgba(41, 98, 255, 0.4); }
    
    /* Signal Banners */
    .signal-banner { padding: 25px; border-radius: 12px; margin-bottom: 25px; text-align: center; animation: fadeIn 1s; }
    .sig-buy { background: linear-gradient(135deg, #004d40 0%, #00695c 100%); border: 1px solid #00e676; box-shadow: 0 0 20px rgba(0, 230, 118, 0.2); }
    .sig-sell { background: linear-gradient(135deg, #b71c1c 0%, #c62828 100%); border: 1px solid #ff5252; box-shadow: 0 0 20px rgba(255, 82, 82, 0.2); }
    .sig-hold { background: linear-gradient(135deg, #37474f 0%, #455a64 100%); border: 1px solid #90a4ae; }
    
    /* Text Typography */
    .big-signal { font-size: 36px; font-weight: 800; letter-spacing: 2px; margin: 0; }
    .sub-signal { font-size: 14px; opacity: 0.8; font-style: italic; }
    .report-text { font-family: 'Courier New', monospace; font-size: 14px; line-height: 1.7; background: #181b21; padding: 20px; border-radius: 8px; border-left: 4px solid #2962ff; }
    
    /* Metrics */
    .kpi-card { background: #1e2228; padding: 15px; border-radius: 8px; border: 1px solid #333; }
    .kpi-val { font-size: 18px; font-weight: bold; color: #fff; }
    .kpi-lbl { font-size: 12px; color: #888; text-transform: uppercase; }
    
    @keyframes fadeIn { from { opacity: 0; transform: translateY(10px); } to { opacity: 1; transform: translateY(0); } }
</style>
""", unsafe_allow_html=True)

# -----------------------------------------------------------------------------
# 2. CONSTANTS & DATA LAYER
# -----------------------------------------------------------------------------
ASSETS = {
    "Indices": {"^NSEI": "NIFTY 50", "^NSEBANK": "BANK NIFTY", "^BSESN": "SENSEX", "^GSPC": "S&P 500"},
    "Crypto": {"BTC-USD": "Bitcoin", "ETH-USD": "Ethereum", "SOL-USD": "Solana"},
    "Stocks": {"RELIANCE.NS": "Reliance", "HDFCBANK.NS": "HDFC Bank", "TCS.NS": "TCS", "AAPL": "Apple", "NVDA": "Nvidia"},
    "Commodities": {"GC=F": "Gold", "SI=F": "Silver"}
}

# -----------------------------------------------------------------------------
# 3. ADVANCED ANALYTICS ENGINE
# -----------------------------------------------------------------------------
class DeepMarketAnalyzer:
    def __init__(self, ticker, df, timeframe):
        self.ticker = ticker
        self.df = df
        self.tf = timeframe
        self.last = df.iloc[-1]
        
    def calculate_metrics(self):
        df = self.df
        
        # --- 1. Z-Score (Mean Reversion) ---
        df['Mean'] = df['Close'].rolling(20).mean()
        df['Std'] = df['Close'].rolling(20).std()
        df['Z_Score'] = (df['Close'] - df['Mean']) / df['Std']
        
        # --- 2. Volatility Regimes ---
        df['TR'] = np.maximum((df['High'] - df['Low']), abs(df['Close'].shift(1) - df['High']))
        df['ATR'] = df['TR'].rolling(14).mean()
        
        # --- 3. EMAs ---
        df['EMA_20'] = df['Close'].ewm(span=20).mean()
        df['EMA_50'] = df['Close'].ewm(span=50).mean()
        df['EMA_200'] = df['Close'].ewm(span=200).mean()
        
        # --- 4. RSI ---
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))
        
        self.df = df
        return df

    def get_market_memory(self):
        """Greedy Search: Finds historical patterns matching current state."""
        df = self.df
        z_current = self.last['Z_Score']
        
        # Search 1: Z-Score Reversion Stats
        # Find all points in history with similar Z-Score (+/- 0.2 tolerance)
        similar_z = df[(df['Z_Score'] > z_current - 0.2) & (df['Z_Score'] < z_current + 0.2)]
        total_occurrences = len(similar_z)
        
        reverted_count = 0
        if total_occurrences > 0:
            for date in similar_z.index:
                # Look 5 bars ahead
                if date in df.index and df.index.get_loc(date) + 5 < len(df):
                    idx = df.index.get_loc(date)
                    future_price = df.iloc[idx+5]['Close']
                    current_price_at_time = df.iloc[idx]['Close']
                    
                    if z_current < -1.5 and future_price > current_price_at_time: # Oversold, did it go up?
                        reverted_count += 1
                    elif z_current > 1.5 and future_price < current_price_at_time: # Overbought, did it go down?
                        reverted_count += 1
                        
        return {
            "z_occurrences": total_occurrences,
            "z_reversions": reverted_count,
            "z_success_rate": (reverted_count / total_occurrences * 100) if total_occurrences > 0 else 0
        }

    def get_seasonality(self):
        """Analyzes Time/Day bias."""
        df = self.df.copy()
        df['Hour'] = df.index.hour
        df['Day'] = df.index.day_name()
        
        # Day of Week Bias
        day_stats = df.groupby('Day')['Close'].pct_change().mean() * 100
        best_day = day_stats.idxmax()
        worst_day = day_stats.idxmin()
        
        # Hour Bias (Liquidity Hunting Times)
        hour_stats = df.groupby('Hour')['Close'].apply(lambda x: x.max() - x.min()).mean()
        most_volatile_hour = hour_stats.idxmax()
        
        return {
            "best_day": best_day,
            "worst_day": worst_day,
            "volatile_hour": most_volatile_hour
        }

    def get_elliott_wave_estimate(self):
        """Heuristic EW: Identifies 1-2 setup and projects Wave 3."""
        df = self.df
        
        # Use simple local min/max for pivots
        n = 5  # number of points to be checked before and after
        df['min'] = df.iloc[argrelextrema(df['Close'].values, np.less_equal, order=n)[0]]['Close']
        df['max'] = df.iloc[argrelextrema(df['Close'].values, np.greater_equal, order=n)[0]]['Close']
        
        # Get last 3 pivots (Low -> High -> Higher Low) for 1-2 setup
        pivots = []
        for i in range(len(df)-1, max(0, len(df)-100), -1):
            if not np.isnan(df['max'].iloc[i]):
                pivots.append(('High', df['max'].iloc[i], df.index[i]))
            if not np.isnan(df['min'].iloc[i]):
                pivots.append(('Low', df['min'].iloc[i], df.index[i]))
            if len(pivots) >= 3:
                break
                
        # We need Low (Start) -> High (Wave 1 Top) -> Higher Low (Wave 2 Bottom)
        # pivots list is reversed (newest first) -> [Wave 2 Low, Wave 1 High, Start Low]
        
        if len(pivots) == 3 and pivots[0][0] == 'Low' and pivots[1][0] == 'High' and pivots[2][0] == 'Low':
            w2_low = pivots[0][1]
            w1_high = pivots[1][1]
            start_low = pivots[2][1]
            
            # Check Validity
            if w2_low > start_low and w2_low < w1_high:
                # Valid 1-2 Setup
                wave_1_height = w1_high - start_low
                wave_3_target = w2_low + (1.618 * wave_1_height)
                
                # Calculate current progress
                curr_price = df.iloc[-1]['Close']
                
                return {
                    "pattern": "Wave 3 Impulse (In Progress)",
                    "w1_start": start_low,
                    "w1_end": w1_high,
                    "w2_end": w2_low,
                    "w3_target": wave_3_target,
                    "status": "Bullish"
                }
        
        # Bearish Setup (High -> Low -> Lower High)
        if len(pivots) == 3 and pivots[0][0] == 'High' and pivots[1][0] == 'Low' and pivots[2][0] == 'High':
            w2_high = pivots[0][1]
            w1_low = pivots[1][1]
            start_high = pivots[2][1]
            
            if w2_high < start_high and w2_high > w1_low:
                wave_1_height = start_high - w1_low
                wave_3_target = w2_high - (1.618 * wave_1_height)
                
                return {
                    "pattern": "Wave 3 Down (In Progress)",
                    "w1_start": start_high,
                    "w1_end": w1_low,
                    "w2_end": w2_high,
                    "w3_target": wave_3_target,
                    "status": "Bearish"
                }

        return {"pattern": "Corrective / Noise", "status": "Neutral", "w3_target": 0}

    def get_support_resistance_age(self):
        """Finds key levels and calculates how long ago they were tested."""
        df = self.df
        window = 20
        recent_high = df['High'].iloc[-window:].max()
        recent_low = df['Low'].iloc[-window:].min()
        
        # Find exact index of these
        high_idx = df['High'].iloc[-window:].idxmax()
        low_idx = df['Low'].iloc[-window:].idxmin()
        
        now = df.index[-1]
        
        def time_diff(t1, t2):
            diff = t2 - t1
            return str(diff).split('.')[0] # Remove microseconds
            
        return {
            "resistance": recent_high,
            "res_age": time_diff(high_idx, now),
            "support": recent_low,
            "sup_age": time_diff(low_idx, now),
            "breakout_check": self.last['Close'] > recent_high
        }

# -----------------------------------------------------------------------------
# 4. DATA FETCHING (Robust & Rate Limited)
# -----------------------------------------------------------------------------
@st.cache_data(ttl=600)
def fetch_multi_timeframe(ticker):
    """Fetches data for multiple TFs. Uses caching to respect API limits."""
    # We fetch a large chunk of granular data and resampling often is cleaner, 
    # but yfinance is easier with direct calls. We add sleep.
    
    data_store = {}
    
    # Map: UI Label -> (Period, Interval)
    configs = [
        ('1m', '7d', '1m'),
        ('5m', '1mo', '5m'),
        ('15m', '1mo', '15m'),
        ('1h', '6mo', '1h'),
        ('1d', '2y', '1d')
    ]
    
    for label, period, interval in configs:
        try:
            df = yf.download(ticker, period=period, interval=interval, progress=False, auto_adjust=True)
            # IST Conversion
            if not df.empty:
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = df.columns.get_level_values(0)
                if df.index.tz is None:
                    df.index = df.index.tz_localize('UTC')
                else:
                    df.index = df.index.tz_convert('UTC')
                df.index = df.index.tz_convert('Asia/Kolkata')
                data_store[label] = df
            time.sleep(1.0) # Rate limit
        except Exception as e:
            pass
            
    return data_store

# -----------------------------------------------------------------------------
# 5. UI & NARRATIVE GENERATION
# -----------------------------------------------------------------------------
def generate_master_summary(ticker, analyses, ratio_val):
    """
    Synthesizes the dictionary of analyses into a cohesive, professional narrative.
    """
    tf_15m = analyses.get('15m')
    tf_1h = analyses.get('1h')
    tf_1d = analyses.get('1d')
    
    # Default to neutral if data missing
    if not tf_1h: return "Insufficient data."

    # 1. Determine Bias
    score = 0
    if tf_1d['metrics'].iloc[-1]['EMA_20'] > tf_1d['metrics'].iloc[-1]['EMA_50']: score += 2
    if tf_1h['ew']['status'] == "Bullish": score += 1
    if tf_15m['metrics'].iloc[-1]['RSI'] < 30: score += 1 # Dip buy in trend
    
    if tf_1d['metrics'].iloc[-1]['EMA_20'] < tf_1d['metrics'].iloc[-1]['EMA_50']: score -= 2
    if tf_1h['ew']['status'] == "Bearish": score -= 1
    
    signal = "HOLD / NEUTRAL"
    if score >= 2: signal = "BUY / ACCUMULATE"
    if score <= -2: signal = "SELL / DISTRIBUTE"
    
    # 2. Extract Key Data Points
    curr_price = tf_15m['metrics'].iloc[-1]['Close']
    z_stats = tf_1h['memory']
    seas = tf_1h['seas']
    sr = tf_15m['sr']
    ew = tf_1h['ew']
    
    # 3. Construct Narrative
    text = f"""
    ### **EXECUTIVE MARKET INTELLIGENCE REPORT: {ticker}**
    
    **1. STRATEGIC SIGNAL: {signal}**
    The automated greedy-search algorithm has analyzed price action across 5 timeframes (1m to 1d) and synthesized a bias score of {score}. 
    Current Price: **{curr_price:.2f}**.
    
    **2. ELLIOTT WAVE & STRUCTURE (1H PRIMARY VIEW)**
    Our pattern recognition engine scans for Golden Ratio setups. On the 1H timeframe, the market is currently forming a **{ew['pattern']}**. 
    {f"The algorithm identifies a valid Wave 1-2 setup. If support holds at {ew['w2_end']:.2f}, the projected Wave 3 target is {ew['w3_target']:.2f}." if ew['status'] != 'Neutral' else "Market structure is currently corrective/undefined. We are waiting for a clear 5-wave impulse to confirm direction."}
    
    **3. HISTORICAL PATTERN MATCHING (GREEDY SEARCH)**
    We searched the database for previous instances where the Z-Score was approximately **{tf_1h['metrics'].iloc[-1]['Z_Score']:.2f}**.
    * **Frequency:** This statistical deviation has occurred **{z_stats['z_occurrences']}** times in the analyzed history.
    * **Outcome:** In **{z_stats['z_success_rate']:.1f}%** of those cases, price reverted to the mean within 5 bars.
    * **Volatility Implication:** Volatility (ATR) is currently {tf_1h['metrics'].iloc[-1]['ATR']:.2f}. Historical data suggests that breakout attempts during this volatility regime have a success rate correlated with volume (check volume manually).
    
    **4. TIME & SEASONALITY DECODED**
    Algorithmic timing analysis reveals specific liquidity behaviors:
    * **Day Bias:** Historically, **{seas['best_day']}s** have shown the highest average returns for this asset, while {seas['worst_day']}s are often bearish or choppy.
    * **Liquidity Window:** The hour of **{seas['volatile_hour']}:00** IST is statistically the most volatile, often serving as a window for liquidity hunting or stop-runs.
    
    **5. SUPPORT/RESISTANCE AGING**
    * **Resistance:** {sr['resistance']:.2f} (Tested **{sr['res_age']}** ago).
    * **Support:** {sr['support']:.2f} (Tested **{sr['sup_age']}** ago).
    * **Analysis:** Levels that haven't been tested for long durations often hold less liquidity. { "The recent breakout above resistance suggests momentum." if sr['breakout_check'] else "Price is contained within recent structure."}
    
    **6. RELATIVE STRENGTH**
    Compared to the benchmark, the Ratio Score is **{ratio_val:.4f}**. { "This asset is showing relative strength." if ratio_val > 1.0 else "This asset is lagging the broader market."}
    """
    
    return text, signal, score

# -----------------------------------------------------------------------------
# 6. MAIN APPLICATION
# -----------------------------------------------------------------------------
def main():
    # Sidebar
    st.sidebar.title("üéõÔ∏è Quant Control")
    cat = st.sidebar.selectbox("Category", list(ASSETS.keys()))
    ticker_name = st.sidebar.selectbox("Asset", list(ASSETS[cat].values()))
    ticker_sym = [k for k,v in ASSETS[cat].items() if v == ticker_name][0]
    
    ratio_sym = st.sidebar.text_input("Benchmark Ticker", "^NSEI")
    
    run = st.sidebar.button("RUN FULL ANALYSIS")
    
    if run:
        with st.spinner(f"üîç AI Agent scanning {ticker_name} across 5 timeframes..."):
            
            # 1. Fetch All Data
            data_map = fetch_multi_timeframe(ticker_sym)
            if not data_map:
                st.error("Failed to fetch data. Try again later.")
                return
            
            # 2. Benchmark Data
            bench_data = fetch_multi_timeframe(ratio_sym)
            ratio_val = 1.0
            if bench_data and '1d' in bench_data and '1d' in data_map:
                try:
                    ratio_val = data_map['1d']['Close'].iloc[-1] / bench_data['1d']['Close'].iloc[-1]
                except: pass

            # 3. Analyze Each Timeframe
            results = {}
            for tf, df in data_map.items():
                engine = DeepMarketAnalyzer(ticker_sym, df, tf)
                df_calc = engine.calculate_metrics()
                
                res = {
                    'metrics': df_calc,
                    'memory': engine.get_market_memory(),
                    'seas': engine.get_seasonality(),
                    'ew': engine.get_elliott_wave_estimate(),
                    'sr': engine.get_support_resistance_age()
                }
                results[tf] = res

            # 4. Generate Narrative
            report, signal, score = generate_master_summary(ticker_name, results, ratio_val)
            
            # --- DISPLAY SECTION ---
            
            # A. Signal Banner
            sig_class = "sig-buy" if "BUY" in signal else "sig-sell" if "SELL" in signal else "sig-hold"
            st.markdown(f"""
            <div class='signal-banner {sig_class}'>
                <p class='sub-signal'>FINAL ALGORITHMIC VERDICT</p>
                <p class='big-signal'>{signal}</p>
                <p class='sub-signal'>Confidence Score: {abs(score)}/5</p>
            </div>
            """, unsafe_allow_html=True)
            
            # B. The Narrative
            col_text, col_chart = st.columns([1, 1.5])
            
            with col_text:
                st.markdown(f"<div class='report-text'>{report}</div>", unsafe_allow_html=True)
                
            with col_chart:
                # C. Advanced Visualization (1H Chart with Waves)
                df_1h = results['1h']['metrics']
                ew_1h = results['1h']['ew']
                
                fig = make_subplots(rows=2, cols=1, shared_xaxes=True, row_heights=[0.7, 0.3], vertical_spacing=0.05)
                
                # Candlestick
                fig.add_trace(go.Candlestick(x=df_1h.index, open=df_1h['Open'], high=df_1h['High'], 
                                             low=df_1h['Low'], close=df_1h['Close'], name='Price'), row=1, col=1)
                
                # EMAs
                fig.add_trace(go.Scatter(x=df_1h.index, y=df_1h['EMA_20'], line=dict(color='yellow', width=1), name='EMA 20'), row=1, col=1)
                fig.add_trace(go.Scatter(x=df_1h.index, y=df_1h['EMA_50'], line=dict(color='cyan', width=1), name='EMA 50'), row=1, col=1)
                
                # Elliott Wave Visuals (If valid)
                if ew_1h['status'] != 'Neutral':
                    # Draw Line from Start -> W1 -> W2 -> Target
                    path_x = [df_1h.index[0], df_1h.index[-1]] # Simplified for plotting logic
                    # Note: Exact coordinate plotting requires complex index matching, simplified here:
                    fig.add_hline(y=ew_1h['w3_target'], line_dash="dot", line_color="lime", annotation_text="Wave 3 Target")
                    fig.add_hline(y=ew_1h['w2_end'], line_dash="dot", line_color="orange", annotation_text="Wave 2 Support")
                
                # Z-Score Subplot
                fig.add_trace(go.Scatter(x=df_1h.index, y=df_1h['Z_Score'], line=dict(color='magenta'), name='Z-Score'), row=2, col=1)
                fig.add_hline(y=2, line_dash='dot', line_color='red', row=2, col=1)
                fig.add_hline(y=-2, line_dash='dot', line_color='green', row=2, col=1)
                
                fig.update_layout(template="plotly_dark", height=600, xaxis_rangeslider_visible=False, title=f"{ticker_name} (1H) - Structure & Volatility")
                st.plotly_chart(fig, use_container_width=True)

            # D. The "Greedy Search" Data Breakdown (No Tables, just Cards)
            st.markdown("### üß¨ Multi-Timeframe DNA")
            
            tfs = ['5m', '15m', '1h', '1d']
            cols = st.columns(len(tfs))
            
            for idx, tf in enumerate(tfs):
                if tf in results:
                    r = results[tf]
                    last_row = r['metrics'].iloc[-1]
                    trend = "UP" if last_row['Close'] > last_row['EMA_50'] else "DOWN"
                    color = "#00e676" if trend == "UP" else "#ff5252"
                    
                    with cols[idx]:
                        st.markdown(f"""
                        <div class='kpi-card' style='border-top: 3px solid {color}'>
                            <div class='kpi-lbl'>{tf} Timeframe</div>
                            <div class='kpi-val'>{trend}</div>
                            <div style='font-size:12px; margin-top:5px;'>
                                RSI: {last_row['RSI']:.0f}<br>
                                Z-Score: {last_row['Z_Score']:.2f}<br>
                                Volatility: {last_row['ATR']:.2f}<br>
                                <i>Supp tested {r['sr']['sup_age']} ago</i>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()

